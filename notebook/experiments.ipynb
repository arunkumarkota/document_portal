{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adf21b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3b800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62371c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, the user is asking for the capital of France. Let me think. I remember from school that France's capital is Paris. But wait, maybe I should verify that. Let me see... Yes, Paris is definitely the capital. It's a major city in Europe, known for landmarks like the Eiffel Tower and the Louvre. I don't think there's any other city that could be confused with it in France. The government is based there, and it's the largest city in the country. I'm pretty confident the answer is Paris. I should just state that clearly.\\n</think>\\n\\nThe capital of France is **Paris**. It is renowned for its cultural, historical, and political significance, as well as iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "model.invoke(\"what is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7d4dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04049589857459068,\n",
       " -0.04480205848813057,\n",
       " -0.03519536182284355,\n",
       " -0.039978861808776855,\n",
       " 0.060929879546165466,\n",
       " -0.0020229103974997997,\n",
       " 0.0018945873016491532,\n",
       " -0.02098630554974079,\n",
       " 0.00390483345836401,\n",
       " 0.07453471422195435,\n",
       " -0.018631644546985626,\n",
       " 0.007455417420715094,\n",
       " -0.005498720332980156,\n",
       " 0.0277552530169487,\n",
       " 0.02441469207406044,\n",
       " -0.04078073427081108,\n",
       " 0.006169943604618311,\n",
       " 0.028121715411543846,\n",
       " 0.047293782234191895,\n",
       " -0.011507853865623474,\n",
       " 0.01713664084672928,\n",
       " 0.008143678307533264,\n",
       " -0.03534378856420517,\n",
       " 0.024886861443519592,\n",
       " 0.018857652321457863,\n",
       " -0.05387217551469803,\n",
       " 0.012068687006831169,\n",
       " -0.04790126904845238,\n",
       " -0.012439027428627014,\n",
       " -0.0018709597643464804,\n",
       " -0.07322046160697937,\n",
       " 0.04119649529457092,\n",
       " 0.003926463890820742,\n",
       " -0.0114192645996809,\n",
       " 0.027035439386963844,\n",
       " -0.05288225784897804,\n",
       " -0.004346303641796112,\n",
       " 0.023509696125984192,\n",
       " -0.00741075212135911,\n",
       " 0.0356050580739975,\n",
       " -0.00813733134418726,\n",
       " -0.009293914772570133,\n",
       " -0.04593217745423317,\n",
       " 0.01098004449158907,\n",
       " -0.010773461312055588,\n",
       " -0.01597120612859726,\n",
       " -0.024269158020615578,\n",
       " 0.06505946815013885,\n",
       " 0.011711898259818554,\n",
       " -0.00990017969161272,\n",
       " 0.01595977507531643,\n",
       " -0.008206325583159924,\n",
       " 0.05414727330207825,\n",
       " 0.015241783112287521,\n",
       " 0.01565716415643692,\n",
       " -0.06717009097337723,\n",
       " 0.013598041608929634,\n",
       " -0.01206971611827612,\n",
       " -0.006766031496226788,\n",
       " 0.02258889749646187,\n",
       " 0.03195677697658539,\n",
       " -0.02795996703207493,\n",
       " 0.004754781257361174,\n",
       " 0.03880754113197327,\n",
       " 0.018016405403614044,\n",
       " -0.056480422616004944,\n",
       " 0.033468831330537796,\n",
       " 0.013152416795492172,\n",
       " 0.08275853842496872,\n",
       " 0.003966258838772774,\n",
       " -0.006652957759797573,\n",
       " -0.04722088947892189,\n",
       " 0.07506217807531357,\n",
       " 0.00833130069077015,\n",
       " 0.024480273947119713,\n",
       " -0.09435391426086426,\n",
       " -0.02432459406554699,\n",
       " 0.06469137221574783,\n",
       " 0.015098823234438896,\n",
       " -0.041668083518743515,\n",
       " -0.01132252812385559,\n",
       " -0.05375457555055618,\n",
       " -0.0857798382639885,\n",
       " -0.031574495136737823,\n",
       " -0.059871990233659744,\n",
       " 0.010076175443828106,\n",
       " -0.05363050475716591,\n",
       " -0.016513127833604813,\n",
       " -0.05007170885801315,\n",
       " 0.05129191651940346,\n",
       " -0.02370879054069519,\n",
       " 0.010461362078785896,\n",
       " 0.07118172198534012,\n",
       " -0.04078911244869232,\n",
       " -0.02913387306034565,\n",
       " 0.06759403645992279,\n",
       " -0.014845332130789757,\n",
       " -0.012595937587320805,\n",
       " 0.07580124586820602,\n",
       " -0.008960776962339878,\n",
       " 0.019636666402220726,\n",
       " -0.02066907100379467,\n",
       " -0.06830579042434692,\n",
       " 0.04878520220518112,\n",
       " 0.03572282940149307,\n",
       " -0.012236935086548328,\n",
       " -0.010849707759916782,\n",
       " 0.05455821380019188,\n",
       " 0.007740095257759094,\n",
       " 0.0630125179886818,\n",
       " -0.07112842798233032,\n",
       " -0.045658886432647705,\n",
       " 0.028484303504228592,\n",
       " 0.05648983642458916,\n",
       " 0.01675332337617874,\n",
       " -0.052414052188396454,\n",
       " -0.028778357431292534,\n",
       " 0.021177630871534348,\n",
       " 0.040066733956336975,\n",
       " 0.02611405774950981,\n",
       " 0.03870968148112297,\n",
       " -0.01853475719690323,\n",
       " 0.04437531530857086,\n",
       " -0.04438794031739235,\n",
       " 0.03159865736961365,\n",
       " -0.016245849430561066,\n",
       " -0.048600293695926666,\n",
       " 0.03397243097424507,\n",
       " -0.0050695547834038734,\n",
       " 0.015177370980381966,\n",
       " 0.00915257353335619,\n",
       " -0.047043170779943466,\n",
       " -0.02225576899945736,\n",
       " -0.0011699205497279763,\n",
       " 0.01987353153526783,\n",
       " 0.03717658296227455,\n",
       " 0.05481540039181709,\n",
       " -0.014918218366801739,\n",
       " 0.04227130860090256,\n",
       " 0.004491603001952171,\n",
       " 0.02655336633324623,\n",
       " 0.06508819013834,\n",
       " 0.0050729988142848015,\n",
       " 0.029402025043964386,\n",
       " -0.019047273322939873,\n",
       " 0.05203188210725784,\n",
       " -0.061927396804094315,\n",
       " -0.03418806567788124,\n",
       " 0.043734438717365265,\n",
       " -0.04346655681729317,\n",
       " -0.07404075562953949,\n",
       " 0.002955378033220768,\n",
       " -0.04757191985845566,\n",
       " -0.026957297697663307,\n",
       " 0.04407310113310814,\n",
       " 0.0046082898043096066,\n",
       " -0.011509543284773827,\n",
       " 0.04934639111161232,\n",
       " 0.02039908431470394,\n",
       " -0.016596553847193718,\n",
       " 0.058120984584093094,\n",
       " 0.01857122592628002,\n",
       " 0.016607575118541718,\n",
       " 0.006390347145497799,\n",
       " -0.0024492843076586723,\n",
       " -0.022973939776420593,\n",
       " 0.03097490966320038,\n",
       " -0.011230415664613247,\n",
       " 0.018612513318657875,\n",
       " -0.0011642066529020667,\n",
       " -0.013932803645730019,\n",
       " 0.017299074679613113,\n",
       " -0.005183328874409199,\n",
       " -0.02756204642355442,\n",
       " 0.007250694558024406,\n",
       " -0.03435893356800079,\n",
       " 0.01172571536153555,\n",
       " -0.023683499544858932,\n",
       " -0.015510759316384792,\n",
       " -0.031465303152799606,\n",
       " 0.007509587332606316,\n",
       " -0.04015637934207916,\n",
       " 0.013057157397270203,\n",
       " 0.04102318733930588,\n",
       " 0.012030666694045067,\n",
       " -0.04409577324986458,\n",
       " 0.02795119769871235,\n",
       " -0.031370554119348526,\n",
       " 0.0020824801176786423,\n",
       " 0.022251205518841743,\n",
       " -0.04768866300582886,\n",
       " -0.011363934725522995,\n",
       " -0.022053701803088188,\n",
       " -0.02086089365184307,\n",
       " -0.03565332666039467,\n",
       " -0.010937473736703396,\n",
       " 0.029957393184304237,\n",
       " -0.014769344590604305,\n",
       " 0.043145839124917984,\n",
       " -0.05224718898534775,\n",
       " -0.024222128093242645,\n",
       " 0.038145340979099274,\n",
       " 0.018341824412345886,\n",
       " -0.028980541974306107,\n",
       " 0.01722531020641327,\n",
       " -0.0041764164343476295,\n",
       " 0.09076888859272003,\n",
       " -0.03722015395760536,\n",
       " -0.05027609318494797,\n",
       " 0.05215790867805481,\n",
       " -0.012998655438423157,\n",
       " 0.008780761621892452,\n",
       " -0.025272613391280174,\n",
       " 0.011743598617613316,\n",
       " 0.04203746095299721,\n",
       " -0.026697078719735146,\n",
       " 0.07385823875665665,\n",
       " 0.010556662455201149,\n",
       " 0.04205450043082237,\n",
       " -0.024556642398238182,\n",
       " -0.04050295427441597,\n",
       " -0.004069443326443434,\n",
       " -0.021989906206727028,\n",
       " -0.010687476955354214,\n",
       " 0.041327763348817825,\n",
       " 0.028045659884810448,\n",
       " -0.00919997040182352,\n",
       " -0.03327375277876854,\n",
       " 0.000798177148681134,\n",
       " -0.013354405760765076,\n",
       " -0.048952922224998474,\n",
       " 0.07362556457519531,\n",
       " 0.029734747484326363,\n",
       " -0.02204304374754429,\n",
       " 0.06452139467000961,\n",
       " -0.0018220851197838783,\n",
       " -0.002310924930498004,\n",
       " 0.02215779386460781,\n",
       " 0.002388277556747198,\n",
       " -0.002533142687752843,\n",
       " -0.02539815939962864,\n",
       " -0.03312728926539421,\n",
       " 0.04245744273066521,\n",
       " 0.024170683696866035,\n",
       " -0.04072323441505432,\n",
       " -0.03723141551017761,\n",
       " -0.027613334357738495,\n",
       " 0.02394406497478485,\n",
       " 0.04481898993253708,\n",
       " 0.04535920172929764,\n",
       " -0.0032587379682809114,\n",
       " -0.0004388995876070112,\n",
       " 0.03285910189151764,\n",
       " 0.01760149747133255,\n",
       " -0.08256322145462036,\n",
       " 0.027649592608213425,\n",
       " -0.06751957535743713,\n",
       " 0.030853813514113426,\n",
       " -0.03653700277209282,\n",
       " 0.03154642507433891,\n",
       " 0.03589324280619621,\n",
       " 0.03334618732333183,\n",
       " 0.038730207830667496,\n",
       " -0.03706396743655205,\n",
       " -0.011471301317214966,\n",
       " -0.03637073561549187,\n",
       " 0.0023318633902817965,\n",
       " -0.05364224314689636,\n",
       " 0.010904532857239246,\n",
       " 0.006715408526360989,\n",
       " 0.03490792214870453,\n",
       " -0.07031109184026718,\n",
       " 0.016439497470855713,\n",
       " 0.036860622465610504,\n",
       " 0.03032566048204899,\n",
       " 0.010677222162485123,\n",
       " -0.05305841565132141,\n",
       " 0.06286440789699554,\n",
       " 0.06270558387041092,\n",
       " -0.07088932394981384,\n",
       " 0.04064720869064331,\n",
       " 0.029234955087304115,\n",
       " -0.0059632412157952785,\n",
       " -0.0190551970154047,\n",
       " -0.0054027121514081955,\n",
       " -0.015841184183955193,\n",
       " -0.04716286435723305,\n",
       " -0.011090202257037163,\n",
       " 0.009548588655889034,\n",
       " -0.06007857620716095,\n",
       " -0.03337058797478676,\n",
       " -0.0807088166475296,\n",
       " 0.032452166080474854,\n",
       " -0.04080561548471451,\n",
       " -0.09995683282613754,\n",
       " 0.000732609536498785,\n",
       " -0.02875148132443428,\n",
       " 0.038151271641254425,\n",
       " 0.03327471390366554,\n",
       " -0.02466670051217079,\n",
       " -0.03210589289665222,\n",
       " 0.004603350069373846,\n",
       " 0.03380708768963814,\n",
       " -0.06126943230628967,\n",
       " 0.030772460624575615,\n",
       " 0.02966260351240635,\n",
       " -0.03604308143258095,\n",
       " -0.06102821230888367,\n",
       " 0.029228761792182922,\n",
       " 0.02995629981160164,\n",
       " 0.04501528665423393,\n",
       " -0.012613361701369286,\n",
       " -0.058307912200689316,\n",
       " -0.03434529900550842,\n",
       " -0.013430471532046795,\n",
       " 0.06727061420679092,\n",
       " -0.01931852474808693,\n",
       " -0.022273380309343338,\n",
       " 0.0172544177621603,\n",
       " 0.028084540739655495,\n",
       " 0.011966834776103497,\n",
       " 0.08038466423749924,\n",
       " 0.02990385890007019,\n",
       " -0.01733776554465294,\n",
       " -0.00425852881744504,\n",
       " 0.005884779617190361,\n",
       " 0.01429015677422285,\n",
       " 0.018236370757222176,\n",
       " 0.015414105728268623,\n",
       " -0.011564589105546474,\n",
       " -0.027001330628991127,\n",
       " 0.03680700808763504,\n",
       " -0.04364212974905968,\n",
       " 0.005125280469655991,\n",
       " 0.010493139736354351,\n",
       " 0.06202792003750801,\n",
       " -0.040758583694696426,\n",
       " 0.02475064806640148,\n",
       " -0.05432722344994545,\n",
       " 0.0058959778398275375,\n",
       " 0.04371344670653343,\n",
       " 0.049989670515060425,\n",
       " -0.019738079980015755,\n",
       " -0.018227912485599518,\n",
       " -0.017926182597875595,\n",
       " -0.017255669459700584,\n",
       " -0.015845123678445816,\n",
       " 0.019173292443156242,\n",
       " 0.09459353983402252,\n",
       " 0.02536124177277088,\n",
       " 0.001187007874250412,\n",
       " 0.08021384477615356,\n",
       " 0.00924182590097189,\n",
       " 0.04623769223690033,\n",
       " -0.0004556997155304998,\n",
       " -0.02065275050699711,\n",
       " 0.05971353501081467,\n",
       " -0.012521890923380852,\n",
       " 0.012784424237906933,\n",
       " -0.05809435993432999,\n",
       " 0.016627559438347816,\n",
       " 0.0310691986232996,\n",
       " -0.03631414473056793,\n",
       " -0.0492539219558239,\n",
       " -0.019370965659618378,\n",
       " -0.020427018404006958,\n",
       " -0.010020453482866287,\n",
       " 0.002172649372369051,\n",
       " 0.018464308232069016,\n",
       " 0.022155364975333214,\n",
       " 0.015383323654532433,\n",
       " 0.010965285822749138,\n",
       " 0.04771880805492401,\n",
       " -0.04354555159807205,\n",
       " 0.056955087929964066,\n",
       " 0.030178379267454147,\n",
       " -0.06719762831926346,\n",
       " -0.014524973928928375,\n",
       " 0.016225548461079597,\n",
       " 0.024253830313682556,\n",
       " -0.04209240898489952,\n",
       " -0.02529408596456051,\n",
       " 0.049950361251831055,\n",
       " 0.02480176091194153,\n",
       " 0.014506431296467781,\n",
       " -0.016044732183218002,\n",
       " 0.04721377417445183,\n",
       " 0.017429251223802567,\n",
       " -0.025577858090400696,\n",
       " 0.05111472308635712,\n",
       " -0.06414662301540375,\n",
       " 0.07264406234025955,\n",
       " 0.0637986958026886,\n",
       " -0.02453370951116085,\n",
       " -0.01815640553832054,\n",
       " -0.02826329879462719,\n",
       " -0.0064158970490098,\n",
       " -0.028589699417352676,\n",
       " 0.02806905098259449,\n",
       " 0.03566247224807739,\n",
       " -0.021823687478899956,\n",
       " -0.07231482863426208,\n",
       " -0.03792164847254753,\n",
       " -0.006366659887135029,\n",
       " -0.013961256481707096,\n",
       " 0.009016234427690506,\n",
       " 0.0014482670230790973,\n",
       " -0.007783473934978247,\n",
       " -0.05828552320599556,\n",
       " 0.01589290425181389,\n",
       " 0.011120520532131195,\n",
       " -0.01777086965739727,\n",
       " 0.021873313933610916,\n",
       " -0.04373500868678093,\n",
       " -0.01964663341641426,\n",
       " -0.009540091268718243,\n",
       " 0.023465441539883614,\n",
       " -0.016643701121211052,\n",
       " -2.818886969180312e-05,\n",
       " 0.06556988507509232,\n",
       " -0.02015743963420391,\n",
       " -0.006127714645117521,\n",
       " 0.004732717759907246,\n",
       " -0.015490321442484856,\n",
       " -0.054713305085897446,\n",
       " -0.06902629882097244,\n",
       " 0.003236546413972974,\n",
       " 0.03797807916998863,\n",
       " 0.03530682623386383,\n",
       " -0.007956664077937603,\n",
       " 0.052497345954179764,\n",
       " 0.021104488521814346,\n",
       " -0.04636838287115097,\n",
       " -0.07017705589532852,\n",
       " -0.021134676411747932,\n",
       " -0.027609067037701607,\n",
       " 0.003762602573260665,\n",
       " 0.023895254358649254,\n",
       " -0.022583821788430214,\n",
       " -0.0057598622515797615,\n",
       " 0.009686781093478203,\n",
       " -0.04235462471842766,\n",
       " 0.03800799697637558,\n",
       " 0.021413663402199745,\n",
       " -0.0774780660867691,\n",
       " 0.0004925738321617246,\n",
       " -0.009664773009717464,\n",
       " -0.017831752076745033,\n",
       " 0.004776636138558388,\n",
       " -0.06284651160240173,\n",
       " 0.02461630292236805,\n",
       " -0.026436761021614075,\n",
       " -0.0016810768283903599,\n",
       " -0.04477179795503616,\n",
       " -0.09151613712310791,\n",
       " -0.010227843187749386,\n",
       " -0.01437322422862053,\n",
       " 0.08218616247177124,\n",
       " -0.054681967943906784,\n",
       " 0.06387192010879517,\n",
       " 0.004982621408998966,\n",
       " -0.00615812698379159,\n",
       " -0.018258098512887955,\n",
       " -0.1149948239326477,\n",
       " 0.07439476251602173,\n",
       " 0.0218903049826622,\n",
       " 0.006960784085094929,\n",
       " -0.006247085984796286,\n",
       " -0.010823355987668037,\n",
       " 0.029611270874738693,\n",
       " 0.025673655793070793,\n",
       " 0.00453235162422061,\n",
       " -0.03215603157877922,\n",
       " -0.04070455580949783,\n",
       " -0.02171097695827484,\n",
       " -0.02638229914009571,\n",
       " -0.05321156606078148,\n",
       " 0.03981984406709671,\n",
       " -0.044293515384197235,\n",
       " 0.01215767115354538,\n",
       " 0.014081534929573536,\n",
       " 0.02260136604309082,\n",
       " 0.01703871227800846,\n",
       " 0.027764135971665382,\n",
       " -0.024178093299269676,\n",
       " 0.037418004125356674,\n",
       " -0.02304765395820141,\n",
       " -0.011035383678972721,\n",
       " -0.056562088429927826,\n",
       " 0.022719377651810646,\n",
       " -0.008883198723196983,\n",
       " -0.02445538341999054,\n",
       " -0.01567012444138527,\n",
       " -0.013430855236947536,\n",
       " -0.0194851066917181,\n",
       " -0.019389895722270012,\n",
       " 0.00421789800748229,\n",
       " 0.034655649214982986,\n",
       " 0.036937110126018524,\n",
       " 0.004344061017036438,\n",
       " -0.026959843933582306,\n",
       " -0.016799381002783775,\n",
       " -0.0024602345656603575,\n",
       " -0.025223901495337486,\n",
       " 0.057808876037597656,\n",
       " -0.09295155853033066,\n",
       " -0.012100448831915855,\n",
       " 0.025417771190404892,\n",
       " -0.02066008187830448,\n",
       " -0.032801464200019836,\n",
       " 0.0006481426535174251,\n",
       " 0.01030695904046297,\n",
       " 0.03199635073542595,\n",
       " 0.04073688015341759,\n",
       " 0.022221416234970093,\n",
       " -0.0029444892425090075,\n",
       " 0.01315197255462408,\n",
       " -0.004565185867249966,\n",
       " 0.018314460292458534,\n",
       " -0.029337801039218903,\n",
       " 0.01444859430193901,\n",
       " -0.025903156027197838,\n",
       " -0.09524020552635193,\n",
       " -0.002099149627611041,\n",
       " -0.029483281075954437,\n",
       " 0.019224995747208595,\n",
       " 0.022705597802996635,\n",
       " 0.053441938012838364,\n",
       " -0.0355648472905159,\n",
       " -0.0009669328574091196,\n",
       " -0.024327477440238,\n",
       " 0.020528599619865417,\n",
       " -0.010461662895977497,\n",
       " -0.02441529929637909,\n",
       " 0.03521616384387016,\n",
       " -0.0035327007062733173,\n",
       " -0.005011484492570162,\n",
       " 0.015951240435242653,\n",
       " 0.028823083266615868,\n",
       " -0.022921429947018623,\n",
       " 0.039546482264995575,\n",
       " 0.02260730229318142,\n",
       " 0.06468944996595383,\n",
       " 0.01812911033630371,\n",
       " -0.03061552345752716,\n",
       " -0.013902192935347557,\n",
       " -0.0061100199818611145,\n",
       " -0.05473892018198967,\n",
       " -0.01829078048467636,\n",
       " 0.034489043056964874,\n",
       " -0.005583097692579031,\n",
       " 0.01991846412420273,\n",
       " 0.010429474525153637,\n",
       " -0.013883198611438274,\n",
       " 0.014815482310950756,\n",
       " -0.02793223038315773,\n",
       " -0.01903199590742588,\n",
       " 0.03920678794384003,\n",
       " 0.01517440751194954,\n",
       " -0.047574203461408615,\n",
       " -0.05044238641858101,\n",
       " 0.0008338171173818409,\n",
       " -0.0022520178463310003,\n",
       " 0.024571724236011505,\n",
       " 0.08084134757518768,\n",
       " 0.025915881618857384,\n",
       " -0.016095496714115143,\n",
       " -0.015557829290628433,\n",
       " 0.06818326562643051,\n",
       " 0.0013206842122599483,\n",
       " -0.010275495238602161,\n",
       " 0.009359994903206825,\n",
       " 0.011058165691792965,\n",
       " 0.031549714505672455,\n",
       " 0.008585716597735882,\n",
       " -0.0233459435403347,\n",
       " 0.00751951988786459,\n",
       " -0.039554886519908905,\n",
       " -0.03721010312438011,\n",
       " -0.009713971987366676,\n",
       " 0.028422679752111435,\n",
       " -0.007892055436968803,\n",
       " -0.0030861650593578815,\n",
       " 0.02596166543662548,\n",
       " 0.008772616274654865,\n",
       " 0.03363153710961342,\n",
       " 0.057541683316230774,\n",
       " 0.1019878014922142,\n",
       " 0.040293339639902115,\n",
       " 0.05504341796040535,\n",
       " -0.03201066330075264,\n",
       " 0.01813836209475994,\n",
       " -0.031475428491830826,\n",
       " 0.0006648820708505809,\n",
       " 0.01786964386701584,\n",
       " 0.0076837497763335705,\n",
       " -0.018364831805229187,\n",
       " -0.028105178847908974,\n",
       " -0.013622824102640152,\n",
       " -0.015150739811360836,\n",
       " 0.05624852702021599,\n",
       " -0.02013879269361496,\n",
       " 0.0044675483368337154,\n",
       " -0.025596806779503822,\n",
       " 0.019242072477936745,\n",
       " 0.05761236697435379,\n",
       " -0.004415152594447136,\n",
       " 0.012549351900815964,\n",
       " -0.0077471681870520115,\n",
       " 0.014663511887192726,\n",
       " 0.027408527210354805,\n",
       " -0.041193991899490356,\n",
       " 0.015324910171329975,\n",
       " -0.017608411610126495,\n",
       " -0.03168262541294098,\n",
       " -0.032555632293224335,\n",
       " 0.07168321311473846,\n",
       " 0.009799330495297909,\n",
       " 0.02118111401796341,\n",
       " -0.04160045087337494,\n",
       " 0.011955291032791138,\n",
       " -0.016465919092297554,\n",
       " 0.031664226204156876,\n",
       " -0.01568051241338253,\n",
       " 0.04173530638217926,\n",
       " 0.03464770317077637,\n",
       " 0.008836270309984684,\n",
       " -0.0027669453993439674,\n",
       " 0.06807421892881393,\n",
       " 0.011814332567155361,\n",
       " 0.05693301931023598,\n",
       " 0.05131593346595764,\n",
       " -0.02981972135603428,\n",
       " 0.01084813941270113,\n",
       " -0.029661012813448906,\n",
       " -0.03380691260099411,\n",
       " -0.05593761056661606,\n",
       " -0.01662912406027317,\n",
       " 0.010600208304822445,\n",
       " -0.02008874900639057,\n",
       " -0.03735111653804779,\n",
       " -0.023617569357156754,\n",
       " 0.007238880731165409,\n",
       " -0.009406358003616333,\n",
       " 0.02834150940179825,\n",
       " 0.10475940257310867,\n",
       " 0.04639627784490585,\n",
       " -0.09643177688121796,\n",
       " -0.056376393884420395,\n",
       " -0.03042629174888134,\n",
       " -0.030333541333675385,\n",
       " -0.007821416482329369,\n",
       " 0.004392584785819054,\n",
       " -0.02059929445385933,\n",
       " 0.04922041296958923,\n",
       " 0.020816422998905182,\n",
       " -0.03358255326747894,\n",
       " -0.0491083599627018,\n",
       " 0.015258957631886005,\n",
       " 0.016441497951745987,\n",
       " -0.015753863379359245,\n",
       " -0.023902930319309235,\n",
       " 0.005782296881079674,\n",
       " 0.008558672852814198,\n",
       " 0.007462106645107269,\n",
       " 0.016703788191080093,\n",
       " -0.030533140525221825,\n",
       " -0.07188694179058075,\n",
       " -0.010673034004867077,\n",
       " 0.0475495420396328,\n",
       " -0.003992817830294371,\n",
       " 0.014632674865424633,\n",
       " 0.02534979209303856,\n",
       " 0.0018633923027664423,\n",
       " 0.042079102247953415,\n",
       " 0.024765687063336372,\n",
       " 0.0022178515791893005,\n",
       " 0.019023988395929337,\n",
       " 0.015153474174439907,\n",
       " 0.009683950804173946,\n",
       " 0.0010971790179610252,\n",
       " -0.0008446355350315571,\n",
       " -0.01728888973593712,\n",
       " 0.004079173784703016,\n",
       " -0.03167934715747833,\n",
       " 0.04870700463652611,\n",
       " 0.030338767915964127,\n",
       " 0.008246907033026218,\n",
       " -0.031554900109767914,\n",
       " -0.05660296976566315,\n",
       " 0.0036534450482577085,\n",
       " -0.014625159092247486,\n",
       " -0.06521178781986237,\n",
       " 0.053864117711782455,\n",
       " 0.0660531297326088,\n",
       " 0.019983068108558655,\n",
       " 0.03262905776500702,\n",
       " 0.007290141191333532,\n",
       " -0.0034917518496513367,\n",
       " 0.021520834416151047,\n",
       " -0.022835295647382736,\n",
       " -0.035963475704193115,\n",
       " -0.04341991990804672,\n",
       " 0.00109879020601511,\n",
       " -0.0013851616531610489,\n",
       " -0.002607055241242051,\n",
       " 0.0372546948492527,\n",
       " 0.03605737164616585,\n",
       " 0.0622076541185379,\n",
       " 0.07573757320642471,\n",
       " 0.035234738141298294,\n",
       " -0.024771325290203094,\n",
       " -0.02295812964439392,\n",
       " 0.023103782907128334,\n",
       " -0.01134227029979229,\n",
       " -0.04811793193221092,\n",
       " 0.024686969816684723,\n",
       " 0.03067523054778576,\n",
       " -0.01713031716644764,\n",
       " 0.09968256950378418,\n",
       " 0.05381736159324646,\n",
       " 0.0518648624420166,\n",
       " 0.062321122735738754,\n",
       " -0.029852405190467834,\n",
       " -0.05626152455806732,\n",
       " 0.08374395966529846,\n",
       " -0.08735101670026779,\n",
       " -0.03338709473609924,\n",
       " -0.030616890639066696,\n",
       " 0.034440141171216965,\n",
       " 0.08644913882017136,\n",
       " -0.0027444453444331884,\n",
       " -0.006311277858912945,\n",
       " -0.01766940765082836,\n",
       " 0.0023246563505381346,\n",
       " 0.08062084764242172,\n",
       " 0.03189488872885704,\n",
       " 0.0393792949616909,\n",
       " 0.0014688537921756506,\n",
       " -0.0906258225440979,\n",
       " -0.02381587214767933,\n",
       " 0.036763254553079605,\n",
       " 0.009490679018199444,\n",
       " 0.04041679576039314,\n",
       " 0.024336544796824455,\n",
       " 0.005297109019011259,\n",
       " -0.016277795657515526,\n",
       " -0.03244338184595108,\n",
       " 0.02116851508617401,\n",
       " -0.07674777507781982,\n",
       " -0.03775396570563316,\n",
       " 0.007768628187477589,\n",
       " -0.02953208051621914,\n",
       " 0.036300890147686005,\n",
       " 0.03283258527517319,\n",
       " -0.013521114364266396,\n",
       " -0.01642083004117012,\n",
       " 0.014644011855125427,\n",
       " -0.015452615916728973,\n",
       " -0.002417708281427622,\n",
       " -0.017770152539014816,\n",
       " -0.043932270258665085,\n",
       " 0.006733616814017296,\n",
       " 0.00907968357205391,\n",
       " 0.010359407402575016,\n",
       " 0.014600984752178192,\n",
       " 0.04979071021080017,\n",
       " -0.006657346151769161]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings_model.embed_query(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75056440",
   "metadata": {},
   "source": [
    "1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38526404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a95434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7dcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20cca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=150,length_function=len)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303da788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "008a6ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5ac4ef7d-2acb-47d9-a3d2-f1329f5de17c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='d321531e-9cda-4c8d-81b4-9e846a833fda', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='1b28ab26-af29-4363-a0d7-05276d5cad8d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='886f1724-4569-4e22-bbe1-7fdff292f632', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='3a5697ec-ba8c-4cd4-bfeb-d899f3445f8f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='ac342e91-989b-4d15-980c-f4d098d86cf4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='5ba1ce5d-3835-49cd-9a73-72475c06d365', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 72, 'page_label': '73'}, page_content='Llama 2\\n7B 0.15 0.30 0.12 0.35 0.25 0.43 0.18 0.38 0.16 0.12 0.29 -0.1313B 0.14 0.35 0.23 0.29 0.23 0.57 0.20 0.52 0.22 0.12 0.29 -0.1734B 0.12 0.16 0.18 0.36 0.35 0.52 0.10 0.54 0.28 0.11 0.30 -0.1970B 0.16 0.21 0.17 0.35 0.30 0.60 0.18 0.67 0.26 0.12 0.30 -0.10\\nFine-tuned\\nChatGPT 0.15 0.22 0.05 0.24 0.31 0.35 0.09 0.42 0.19 0.09 0.23 0.06MPT-instruct 7B 0.13 0.29 0.12 0.34 0.35 0.53 0.28 0.56 0.27 0.02 0.32 -0.12Falcon-instruct 7B 0.11 0.21 0.21 0.28 0.34 0.23 0.31 0.45 0.23 0.22 0.29 -0.27'),\n",
       " Document(id='5887d5c8-7aff-4e0c-8dfa-5688681f2d85', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(id='742c6117-4503-402c-99a3-90126814b585', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'),\n",
       " Document(id='3e3e9904-bd60-4b3e-a317-5c75ccd24531', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/arunkota/Documents/MLOPS/document_portal/notebook/data/sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Llama 2\\n7B 77.4 78.8 48.3 77.2 69.2 75.2 45.9 58.6 57.8 45.3\\n13B 81.7 80.5 50.3 80.7 72.8 77.3 49.4 57.0 67.3 54.8\\n34B 83.7 81.9 50.9 83.3 76.7 79.4 54.5 58.2 74.3 62.6\\n70B 85.0 82.8 50.7 85.3 80.2 80.2 57.4 60.2 78.5 68.9\\nTable 20: Performance on standard benchmarks.\\nHuman-Eval MBPP\\npass@1 pass@100 pass@1 pass@80\\nMPT 7B 18.3 - 22.6 -\\n30B 25.0 - 32.8 -\\nFalcon 7B 0.0 - 11.2 -\\n40B 0.6 - 29.8 -\\nLlama 1\\n7B 10.5 36.5 17.7 56.2\\n13B 15.8 52.5 22.0 64.0\\n33B 21.7 70.7 30.2 73.4\\n65B 23.7 79.3 37.7 76.8')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 fine tunring benchmark expriements\", k =10)\n",
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a54a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\" \n",
    "    Answer the question based on the context provided below.\n",
    "    If the context does not contain sufficient information, respond with :\n",
    "    \"I dont have enough information about this.\"\n",
    "    Context :{context}\n",
    "    Question : {question}\n",
    "    Answer :\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd36cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f59e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e61d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e36581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edaabce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s tackle this question about the Llama 2 fine-tuning benchmark experiments. First, I need to look at the context provided to see what information is available.\\n\\nThe context has several tables with numbers, but it\\'s not very clearly formatted. Let me parse through it step by step. The first part mentions different Llama 2 models with varying parameters (7B, 13B, 34B, 70B) followed by a series of numbers. Then there\\'s a section labeled \"Fine-tuned\" with a 65B model and a bunch of numbers. After that, there\\'s another section with Llama 2 models again and more numbers, followed by \"Fine-tuned\" and some other models like ChatGPT, MPT-instruct, Falcon-instruct, each with their own numbers. Then a table labeled \"Table 3\" with performance on academic benchmarks.\\n\\nThe user is asking specifically about the Llama 2 fine-tuning benchmark experiments. The context does mention \"Fine-tuned\" sections for Llama 2 models. For example, under the \"Fine-tuned\" part after the initial Llama 2 data, there\\'s a line for Llama 2 65B with numbers. However, the numbers here are much higher than the previous ones, which might indicate different metrics. The second \"Fine-tuned\" section includes ChatGPT and other models, which could be part of the benchmark comparisons.\\n\\nLooking at Table 3, it shows performance on grouped academic benchmarks for open-source base models, including Llama 2 at different parameter sizes (7B, 13B, 34B, 70B) with percentages. The numbers in Table 3 seem to represent performance metrics like accuracy or F1 scores across different benchmarks. The context also mentions other models like MPT-instruct and Falcon-instruct for comparison.\\n\\nHowever, the problem is that the context is quite jumbled and not well-structured. The numbers are listed without clear labels for what each column represents. The user is asking for information on the fine-tuning experiments of Llama 2, but the context provided doesn\\'t clearly explain the setup, methodology, or specific results of these experiments. The tables have numerical data, but without column headers or descriptions, it\\'s hard to interpret what each number signifies. \\n\\nAdditionally, the mention of \"Fine-tuned\" in the context might refer to different versions of the models after fine-tuning, but without more details on the benchmarks used, the tasks evaluated, or the evaluation metrics, it\\'s impossible to provide a detailed answer. The user might be expecting information on how Llama 2 models performed after fine-tuning compared to other models or their base versions, but the context lacks the necessary information to answer that comprehensively.\\n\\nTherefore, based on the provided context, while there are numerical results related to Llama 2 models and fine-tuned versions, the absence of clear labels, descriptions, and sufficient context about the experiments means that I can\\'t provide a coherent answer. The user needs more detailed information about the benchmarks, the tasks, and the metrics used, which isn\\'t present here.\\n</think>\\n\\nI don\\'t have enough information about this.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tll me about the llama2 finetuning benchmark experiements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07001a6a",
   "metadata": {},
   "source": [
    "Take 10 PDF keep in the same directory and create RAG on top of it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
